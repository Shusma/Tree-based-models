{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "## Majority Voting\n",
    "\n",
    "[<img src=\"https://www.researchgate.net/publication/324014302/figure/fig2/AS:644424015040514@1530654066950/Majority-voting-algorithm.png\">](https://www.researchgate.net/publication/324014302/figure/fig2/AS:644424015040514@1530654066950/Majority-voting-algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P_f = mode\\{C_1(x), C_2(x), .. C_m(x)\\}$$  where  $C_i(x) = P_i$\n",
    "\n",
    "### Why Majority Vote?\n",
    "- assume $m$ independent classifiers with base error rate $\\epsilon$ \n",
    "- *independent* means errors are not correlated\n",
    "- assume binary classification \n",
    "- assume the error rate is better than random guessing < 0.5\n",
    "- $\\forall \\epsilon_i \\in \\{\\epsilon_1, \\epsilon_2, .. \\epsilon_m\\}, \\epsilon_i < 0.5 $ \n",
    "\n",
    "The probability that we make a wrong prediction via the ensemble if $k$ classifiers predict the same class label (pmf of a binomial distribution).\n",
    "\n",
    "$$ P(k) = \\frac{m!}{(m-k)!k!} \\epsilon^k (1-\\epsilon)^{m-k} $$ \n",
    "\n",
    "Ensemble Error: Cumulative probability distribution\n",
    "\n",
    "$$ \\epsilon_{ens} = \\sum_k^m \\frac{m!}{(m-k)!k!} \\epsilon^k (1-\\epsilon)^{m-k} $$\n",
    "\n",
    "### Soft Voting\n",
    "\n",
    "$$ \\hat{y} = argmax_j \\sum_{i=1}^m w_i p_{i,j}$$\n",
    "\n",
    "where \n",
    "\n",
    "- $w_i$ is an optional weighing parameter $w_i = \\frac{1}{m}, \\quad$  $\\forall w_i \\in \\{w_1,...w_m\\} $\n",
    "\n",
    "- $p_{i,j}$ is the predicted class membership probability of the $i{th}$ classifier for class label $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 84 28 38\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, [0, 1, 2, 3]], iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.96 [Classifier 1]\n",
      "Validation Accuracy: 0.82 [Classifier 2]\n",
      "Validation Accuracy: 1.00 [Classifier 3]\n",
      "Validation Accuracy: 1.00 [Ensemble]\n",
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "clf1 = DecisionTreeClassifier(random_state=1)\n",
    "clf2 = DecisionTreeClassifier(random_state=1, max_depth=1)\n",
    "clf3 = DecisionTreeClassifier(random_state=1, max_depth=3)\n",
    "eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1, 1, 1])\n",
    "\n",
    "labels = ['Classifier 1', 'Classifier 2', 'Classifier 3', 'Ensemble']\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], labels):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Validation Accuracy: %0.2f [%s]\" % (clf.score(X_val, y_val), label))\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % eclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.96 [Classifier 1]\n",
      "Validation Accuracy: 0.82 [Classifier 2]\n",
      "Validation Accuracy: 1.00 [Classifier 3]\n",
      "Validation Accuracy: 1.00 [Ensemble]\n",
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "clf1 = DecisionTreeClassifier(random_state=1)\n",
    "clf2 = DecisionTreeClassifier(random_state=1, max_depth=1)\n",
    "clf3 = DecisionTreeClassifier(random_state=1, max_depth=3)\n",
    "eclf = VotingClassifier(estimators=[('dt1', clf1), ('dt2', clf2), ('dt3', clf3)], weights=[1,1,1])\n",
    "\n",
    "labels = ['Classifier 1', 'Classifier 2', 'Classifier 3', 'Ensemble']\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], labels):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Validation Accuracy: %0.2f [%s]\" % (clf.score(X_val, y_val), label))\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % eclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Let m be the number of bootstrap samples\n",
    "\n",
    "**for** $i=1$ to $m$ **do**:\n",
    "- Draw bootstrap sample of size $n$, $D_i$\n",
    "- Train base classifier $C_i$ on $D_i$\n",
    "\n",
    "$$ P_f = mode\\{C_1(x), C_2(x), .. C_n(x)\\}$$  where  $C_i(x) = P_i$\n",
    "\n",
    "**Bootstrap sampling** \n",
    "\n",
    "Drawing data points from the dataset with replacement until we reach the $n$ training set size. \"Replacement\" means that some records will be duplicated in the bootstrap dataset. Each datapoint is drawn independently from the training set by randomly sampling with a uniform distribution ($\\frac{1}{n}$ training set size).\n",
    "\n",
    "This is a way of enforcing **complexity control** and is often an alternative to cross-validation. \n",
    "\n",
    "The records that are not chosen are referred to as the OOB (out of bag) samples. \n",
    "\n",
    "$$ P(not\\_chosen) = \\Bigg(1-\\frac{1}{n}\\Bigg)^n $$\n",
    "\n",
    "$$ n \\to \\infty \\quad P(not\\_chosen) = \\frac{1}{e} \\approx 0.368 $$\n",
    "\n",
    "$$ P(chosen) = 1 - \\Bigg(1-\\frac{1}{n}\\Bigg)^n \\approx 0.632 $$\n",
    "\n",
    "i.e. only 63.2% of the bootstrap sample is unique.\n",
    "\n",
    "> - The classifier $C_i$ are usually unpruned, deep decision trees.\n",
    "> - The step of computing $m$ bootstrap samples and their respective decision trees & predictions can be PARALLELISED (n_jobs parameter in sklearn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://www.oreilly.com/library/view/python-machine-learning/9781787125933/graphics/B07030_07_06.jpg\">](https://www.oreilly.com/library/view/python-machine-learning/9781787125933/graphics/B07030_07_06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance trade-off \n",
    "\n",
    "$$Loss = Bias + Variance + Noise $$\n",
    "\n",
    "> - Best case scenario: **Low bias & Low Variance**\n",
    "- Overfitting: **Low Bias & High Variance** \n",
    "- Underfitting: **High Bias & Low Variance** \n",
    "- Worst case scenario: **High bias & High Variance**\n",
    "\n",
    "\n",
    "Typically, over-fitting arises as a result of the bias-variance trade off with over-fitted models suffering from a high variance error (highly sensitive to changes in training data) and a low bias error (few assumptions are made about underlying data). This would make the model too complex and specific to the features in the training data set, fitting their random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/978/1*CgIdnlB6JK8orFKPXpc7Rg.png\" alt=\"biasv\" width=\"600\"/>\n",
    "<table><tr>\n",
    "<td> <img src=\"http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp_files/high-bias-plot.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp_files/varianceplot.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-variance plot above shows different unpruned decision tree models, each fit to a different bootstrapped training set. It is evident that these hypotheses fit the training data very closely and the variance is very high, since on average, a prediction differs a lot from the expectation value of the prediction. \n",
    "\n",
    "However, if we would consider the expectation over training sets, the average hypothesis would fit the true function perfectly (given that the noise is unbiased and has an expected value of 0).  \n",
    "\n",
    "> An individual, unpruned decision tree will “have high variance”. Averaging such high variance models will actually result in good predictions. A bagging model has a lower variance than the individual trees and therefore is less prone to overfitting.\n",
    "\n",
    "### Mathematical Aside: Formal definition of Bias-Variance trade off\n",
    "\n",
    "Assume we have a point estimator $\\hat{\\theta}$ of some parameter or function $\\theta$. Then, the bias is commonly defined as the difference between the expected value of the estimator and the parameter that we want to estimate:\n",
    "\n",
    "$$ Bias = E[\\hat{\\theta}] - \\theta $$\n",
    "\n",
    "If the bias is larger than zero, we can say that the estimator is positively biased, if the bias is smaller than zero, the estimator is negatively biased, and if the bias is exactly zero, the estimator is unbiased. \n",
    "\n",
    "Similarly, we define the variance as the difference between the expected value of the squared estimator minus the squared expectation of the estimator:\n",
    "\n",
    "$$ Var(\\hat{\\theta}) = E[\\hat{\\theta}^2] - \\Big(E[\\hat{\\theta}]\\Big)^2  = E[(E[\\hat{\\theta}] - \\hat{\\theta})^2]$$\n",
    "\n",
    "**Key definitions** \n",
    "\n",
    "- the true or target function as $ y = f(x) $,\n",
    "- the predicted target value as $\\hat{y} = \\hat{f}(x) = C(x)$,\n",
    "- the squared loss as $S = (y - \\hat{y})^2 $\n",
    "\n",
    "$$ S = (y - \\hat{y})^2 = (y - E[\\hat{y}] + E[\\hat{y}] - \\hat{y})^2 $$\n",
    "\n",
    "Let $ a = (y - E[\\hat{y}])$ and $ b = (E[\\hat{y}] - \\hat{y})$. Therefore, using $(a+b)^2 = a^2 + b^2 - 2ab$.\n",
    "\n",
    "$$ S = (y - E[\\hat{y}])^2 + (E[\\hat{y}] - \\hat{y})^2 - 2(y - E[\\hat{y}])(E[\\hat{y}] - \\hat{y}) $$\n",
    "\n",
    "Next, take the expection on both sides:\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\begin{split} \n",
    "E[S]\n",
    "& = E[(y - \\hat{y})^2] \\\\\n",
    "& = E\\Big[(y - E[\\hat{y}])^2 + (E[\\hat{y}] - \\hat{y})^2 - 2(y - E[\\hat{y}])(E[\\hat{y}] - \\hat{y})\\Big] \\\\\n",
    "& = E\\Big[(y - E[\\hat{y}])^2\\Big] + E\\Big[(E[\\hat{y}] - \\hat{y})^2\\Big] - E\\Big[2(y - E[\\hat{y}])(E[\\hat{y}] - \\hat{y})\\Big] \\\\\n",
    "& = (y - E[\\hat{y}])^2 + E\\Big[(E[\\hat{y}] - \\hat{y})^2\\Big] \\\\\n",
    "& = [Bias]^2 + Variance\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "The \"$2ab$\" term actually disappears when the expectation is taken as shown below.\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\begin{split} \n",
    "E\\Big[2(y - E[\\hat{y}])(E[\\hat{y}] - \\hat{y})\\Big]  \n",
    "& = 2E\\Big[(y - E[\\hat{y}])(E[\\hat{y}] - \\hat{y})\\Big] \\\\ \n",
    "& =  2(y - E[\\hat{y}])E\\Big[(E[\\hat{y}] - \\hat{y})\\Big] \\\\\n",
    "& = 2(y - E[\\hat{y}])(E[E[\\hat{y}]] - E[\\hat{y}]) \\\\\n",
    "& = 2(y - E[\\hat{y}])(E[\\hat{y}] - E[\\hat{y}]) \\\\\n",
    "& = 0\n",
    "\\end{split}\n",
    "\\end{equation} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit learn demo - Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Accuracy: 0.95\n",
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              random_state=1,\n",
    "                              max_depth=None)\n",
    "\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=tree,\n",
    "                        n_estimators=500,\n",
    "                        oob_score=True,\n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False,\n",
    "                        n_jobs=1,\n",
    "                        random_state=1)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "    \n",
    "print(\"OOB Accuracy: %0.2f\" % bag.oob_score_)\n",
    "print(\"Test Accuracy: %0.2f\" % bag.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "Random Forest = Bagging w/ Decision Trees + Random feature subsets\n",
    "\n",
    "At each node, a random tree only sees a random subset of features. This subset of features is chosen by $log_2p + 1$, where $p$ is the no. of input features. Usually decision trees see all features at each node.\n",
    "\n",
    "Let $m$ be the no. of bootstrap samples\n",
    "1. For $i=1$  to $m$:\n",
    "     - Draw a bootstrap sample $D_i$ of size $n$,  from the training data\n",
    "     - Grow a random-forest tree $RT_i$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimium node size $node_{min}$ is reached. \n",
    "         - Select $q$ variables at random from the $p$ input variables.\n",
    "         - Pick the best variable/split-point among the $q$.\n",
    "         - Split the node into two daughter nodes.\n",
    "2. Output  the ensemble of trees $\\{RT_i\\}_1^m$\n",
    "\n",
    "To make a prediction at a new point x:\n",
    "\n",
    "*Classification:* $\\hat{P_{rf}} = majority\\_vote\\{\\hat{P_i}\\}_1^m $, where $\\hat{P_i}$ is the class prediction of the $i^{th}$ random-forest tree.\n",
    "\n",
    "*Regression:* $\\hat{f}_{rf} = \\frac{1}{m} \\sum_{i=1}^m RT_i(x)$\n",
    "\n",
    "### Randomisation \n",
    "\n",
    "#### Feature Importance Vs Random Forests\n",
    "While random forests are naturally less interpretable than individual decision trees, where we can trace a decision via a rule sets, it is possible (and common) to compute the so-called “feature importance” of the inputs – that means, we can infer how important a feature is for the overall prediction.\n",
    "Randomised node optimistaion: If $T$ is the entire set of all possible parameters $\\theta$ then when training the $j^{th}$ node we only make available a small subset $T_j \\subset T$ of such values.\n",
    "\n",
    "$$ \\theta_j^* = argmax_{\\theta_j \\in T_j} I_j $$\n",
    "\n",
    "\n",
    "### (Loose) Upper Bound for Generalisation Error\n",
    "\n",
    "$$ PE \\leq \\frac{\\bar{\\rho}\\times(1-s^2)}{s^2} $$\n",
    "\n",
    "PE is the prediction error, $\\bar{\\rho}$ is the average correlation among trees i.e. similarity among trees, $s$ is the \"strength\" of the ensemble i.e. how good the trees are.\n",
    "\n",
    "The lower the correlation between the trees, the lower the prediction error. Similarly, the higher the \"strength\" of the ensemble, the lower the error. \n",
    "\n",
    "There is a trade-off between this and randomisation. \n",
    "- Larger the similarity, the smaller the benefit of having an ensemble\n",
    "- However, introducing randomness to reduce correlation and create different trees will decline performance - as the optimal features might not be considered in the nodes.\n",
    "\n",
    "In **Bagging**, the trees themselves generally perform better as each node sees the complete feature set BUT the downside is the trees are generally more similar, so the effect of the ensemble is unremarkable.\n",
    "\n",
    "In **Random Forests**, the trees themselves are weaker, but they are less similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100,\n",
    "                                random_state=1)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "print(\"Test Accuracy: %0.2f\" % forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Stability Index \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Feature Importance\n",
    "\n",
    "#### Sklearn Implementation\n",
    "\n",
    "Usually measured as\n",
    "- impurity decrease (Gini, Entropy) for a given node/feature decision\n",
    "- weighted by number of examples at that node\n",
    "- averaged over all trees\n",
    "- then normalize so that sum of feature importances sum to 1\n",
    "\n",
    "(Unfair for variables with many vs few values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(range(X_train.shape[1]), forest.feature_importances_)\n",
    "ax.set_xlim([-1, X.shape[1]])\n",
    "ax.set_xticks(range(X_train.shape[1]))\n",
    "ax.set_xticklabels(['sepal length', 'sepal width', 'petal length', 'petal width'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance via Permutation\n",
    "\n",
    "#### Permutation Test\n",
    "\n",
    "- A nonparametric test procedure to test the null hypothesis that two different groups come from the same distribution\n",
    "- Can be used for significance or hypothesis testing w/o requiring to make any assumptions about the sampling distribution (e.g., it doesn't require the samples to be normal distributed).\n",
    "- Under the null hypothesis (treatment = control), any permutations are equally likely\n",
    "- Note that there are $(n+m)!$ permutations, where $n$ is the number of records in the treatment sample, and $m$ is the number of records in the control sample\n",
    "- For a two-sided test, we define the alternative hypothesis that the two samples are different (e.g., treatment != control)\n",
    "\n",
    "\n",
    "#### Method\n",
    "\n",
    "1. Compute the difference (eg: mean) of sample $x$ (size $n$) and sample $y$ (size $m$)\n",
    "2. Combine all measurements into a single dataset\n",
    "3. Draw a permuted dataset from all possible permutations of the dataset in 2.\n",
    "4. Divide the permuted dataset into two datasets $x'$ and $y'$ of size $n$ and $m$, respectively\n",
    "5. Compute the difference (eg: mean) of sample $x'$ and sample $y'$ and record this difference\n",
    "6. Repeat steps 3-5 until all permutations are evaluated\n",
    "7. Return the **p-value** as the number of times the recorded differences were more extreme than the original difference from 1., then divide this number by the total number of permutations\n",
    "\n",
    "Here, the **p-value** is defined as the **probability, given the null hypothesis (no difference between the samples) is true, that we obtain results that are at least as extreme as the results we observed (i.e., the sample difference from step 1).**\n",
    "\n",
    "$$ p(t > t_0) = \\frac{1}{(n+m)!} \\sum_{j=1}^{(n+m)!} I(t_j > t_0) $$\n",
    "\n",
    "where $t_0$ is the observed value of the test statistic, and $t$ is the t-value, the statistic computed from the resamples and I is the Indicator function.\n",
    "\n",
    "#### Mlxtend Wrapper for Feature Importance via Permutation\n",
    "\n",
    "[http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/](http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/)\n",
    "\n",
    "1. Take a model that was fit to the training set\n",
    "2. Estimate the predictive performance of the model on an independent dataset (e.g., validation dataset) and record it as the baseline performance\n",
    "3. For each feature i:\n",
    "    - randomly permute feature column i in the original dataset\n",
    "    - record the predictive performance of the model on the dataset with the permuted column\n",
    "    - compute the feature importance as the difference between the baseline performance (step 2) and the performance on the permuted dataset\n",
    "    - Repeat this loop exhaustively (all combinations) or a large number of times and compute the feature importance as the average difference\n",
    "    \n",
    "#### Column drop variant\n",
    "For each feature column i:\n",
    "1. temporarily remove column\n",
    "2. fit model to reduced dataset\n",
    "3. compute validation set performance and compare to before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.10078947, 0.13763158])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.evaluate import feature_importance_permutation\n",
    "\n",
    "imp_vals, _ = feature_importance_permutation(\n",
    "    predict_method=forest.predict, \n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    metric='accuracy',\n",
    "    num_rounds=100,\n",
    "    seed=1)\n",
    "\n",
    "imp_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(range(X_train.shape[1]), imp_vals)\n",
    "ax.set_xlim([-1, X.shape[1]])\n",
    "ax.set_xticks(range(X_train.shape[1]))\n",
    "ax.set_xticklabels(['sepal length', 'sepal width', 'petal length', 'petal width'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Random Forest Feature Importance \"Method B\"\n",
    "\n",
    "#### Out-of-bag accuracy:\n",
    "- During training, for each tree, make prediction for OOB sample (~1/3 of the training data)\n",
    "- Based on those predictions where feature i was OOB, compute label via majority vote among the trees that did not use feature i during model fitting\n",
    "- The proportion over all examples where the prediction (by majority vote) is correct is the OOB accuracy estimate\n",
    "\n",
    "#### Out-of-bag feature importance via permutation:\n",
    "- Count votes for correct class\n",
    "- Given feature i, permute this feature in OOB examples of a tree\n",
    "- Compute the number of correct votes after permutation from the number of votes before permutation for given tree\n",
    "- Repeat for all trees in the random forest and average the importance - Repeat for other features -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomised Trees (ExtraTrees)\n",
    "\n",
    "Compared to regular random forests, the ExtraTrees algorithm selects a random feature at each decision tree nodes for splitting; hence, it is very fast because there is no information gain computation and feature comparison step.\n",
    "\n",
    "Intuitively, one might say that ExtraTrees have another “random component” (compared to random forests) to further reduce the correlation among trees – however, it might decrease the strength of the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "forest = ExtraTreesClassifier(n_estimators=100,\n",
    "                              random_state=1)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(range(X_train.shape[1]), forest.feature_importances_)\n",
    "ax.set_xlim([-1, X.shape[1]])\n",
    "ax.set_xticks(range(X_train.shape[1]))\n",
    "ax.set_xticklabels(['sepal length', 'sepal width', 'petal length', 'petal width'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
